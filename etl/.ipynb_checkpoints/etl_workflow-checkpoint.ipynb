{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7437294",
   "metadata": {},
   "source": [
    "ETL Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ae567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Extract Step...\n",
      "Synthetic data has been saved to ../data/iam_policies.csv\n",
      "Pipeline execution completed.\n"
     ]
    }
   ],
   "source": [
    "!python run_all.py --extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a1b0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# EXTRACT data\n",
    "\n",
    "# Extract data\n",
    "def extract_data():\n",
    "\n",
    "    # Number of rows to generate\n",
    "    n_samples = 10\n",
    "\n",
    "    # Generate data based on the specification\n",
    "    policy_ids = [f'P100{i+1}' for i in range(n_samples)]\n",
    "    user_ids = [f'U00{i+1}' for i in range(n_samples)]\n",
    "    roles_data = np.random.choice(['Admin', 'User', 'Manager'], size=n_samples)\n",
    "    plan_types_data = np.random.choice(['Enterprise', 'Standard', 'Basic'], size=n_samples)\n",
    "    monthly_rates = np.random.choice([50, 120, 320, 520], size=n_samples)\n",
    "    premium_values_data = np.random.choice(['Yes', 'No'], size=n_samples)\n",
    "    regions_data = np.random.choice(['US', 'EU', 'APAC'], size=n_samples)\n",
    "    login_counts = np.random.randint(20, 800, size=n_samples)\n",
    "    last_login_days = np.random.randint(1, 400, size=n_samples)\n",
    "\n",
    "    # Dynamically generate encrypted' and 'non-encrypted' data_type\n",
    "    data_types = np.random.choice(['encrypted', 'non-encrypted'], size=n_samples, p=[0.3, 0.7])  # 30% encrypted, 70% non-encrypted\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'policy_id': policy_ids,\n",
    "        'user_id': user_ids,\n",
    "        'role': roles_data,\n",
    "        'plan_type': plan_types_data,\n",
    "        'monthly_rate': monthly_rates,\n",
    "        'premium': premium_values_data,\n",
    "        'region': regions_data,\n",
    "        'login_count': login_counts,\n",
    "        'last_login_days': last_login_days,\n",
    "        'data_type': data_types\n",
    "    })\n",
    "\n",
    "    # Ensure the 'premium' column is boolean (True/False)\n",
    "    df['premium'] = df['premium'].map({'Yes': True, 'No': False})\n",
    "\n",
    "    # Save the new generated data to a CSV file\n",
    "    output_file = '../data/iam_policies.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Verfiy new data file has been saved\n",
    "    print(f\"Synthetic data has been saved to {output_file}\")\n",
    "\n",
    "    # Return the DataFrame for further processing\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a8d24",
   "metadata": {},
   "source": [
    "ETL Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a57023d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Extract Step...\n",
      "Synthetic data has been saved to ../data/iam_policies.csv\n",
      "Running Transform Step...\n",
      "Initial DataFrame head:\n",
      "  policy_id user_id     role  ... login_count  last_login_days      data_type\n",
      "0     P1001    U001  Manager  ...         666              346      encrypted\n",
      "1     P1002    U002    Admin  ...          40               53  non-encrypted\n",
      "2     P1003    U003  Manager  ...         186              386  non-encrypted\n",
      "3     P1004    U004  Manager  ...         293              340  non-encrypted\n",
      "4     P1005    U005    Admin  ...         407               92  non-encrypted\n",
      "\n",
      "[5 rows x 10 columns]\n",
      "Rows to be reshaped:\n",
      "  policy_id user_id     role  ... login_count  last_login_days      data_type\n",
      "0     P1001    U001  Manager  ...         666              346      encrypted\n",
      "1     P1002    U002    Admin  ...          40               53  non-encrypted\n",
      "2     P1003    U003  Manager  ...         186              386  non-encrypted\n",
      "3     P1004    U004  Manager  ...         293              340  non-encrypted\n",
      "4     P1005    U005    Admin  ...         407               92  non-encrypted\n",
      "\n",
      "[5 rows x 10 columns]\n",
      "Unique regions before reshaping: 3\n",
      "Missing values in 'region' column before reshaping: 0\n",
      "Attempting to reshape data with 3 unique regions.\n",
      "Reshaped DataFrame head:\n",
      "        login_count  monthly_rate\n",
      "region                           \n",
      "APAC          408.4    158.000000\n",
      "EU            333.0    363.333333\n",
      "US            298.0    320.000000\n",
      "Encrypting the data...\n",
      "Data after encryption:\n",
      "  policy_id user_id     role  ... login_count last_login_days      data_type\n",
      "0     P1001    U001  Manager  ...        NjY2            MzQ2      encrypted\n",
      "1     P1002    U002    Admin  ...          40              53  non-encrypted\n",
      "2     P1003    U003  Manager  ...         186             386  non-encrypted\n",
      "3     P1004    U004  Manager  ...         293             340  non-encrypted\n",
      "4     P1005    U005    Admin  ...         407              92  non-encrypted\n",
      "\n",
      "[5 rows x 10 columns]\n",
      "Data before cleaning and conversion saved to '../data/transform_before_cleaning.csv'\n",
      "Skipping numeric conversion for 'monthly_rate' due to 'encrypted' data type.\n",
      "Skipping numeric conversion for 'login_count' due to 'encrypted' data type.\n",
      "Skipping numeric conversion for 'last_login_days' due to 'encrypted' data type.\n",
      "Data after cleaning and conversion to numeric:\n",
      "  policy_id user_id     role  ... login_count last_login_days      data_type\n",
      "0     P1001    U001  Manager  ...        NjY2            MzQ2      encrypted\n",
      "1     P1002    U002    Admin  ...          40              53  non-encrypted\n",
      "2     P1003    U003  Manager  ...         186             386  non-encrypted\n",
      "3     P1004    U004  Manager  ...         293             340  non-encrypted\n",
      "4     P1005    U005    Admin  ...         407              92  non-encrypted\n",
      "\n",
      "[5 rows x 10 columns]\n",
      "Data after cleaning and conversion saved to '../data/transform_after_cleaning.csv'\n",
      "Data before dropping NaNs: 10 rows.\n",
      "Data after dropping NaNs:\n",
      "  policy_id user_id     role  ... login_count last_login_days      data_type\n",
      "0     P1001    U001  Manager  ...        NjY2            MzQ2      encrypted\n",
      "1     P1002    U002    Admin  ...          40              53  non-encrypted\n",
      "2     P1003    U003  Manager  ...         186             386  non-encrypted\n",
      "3     P1004    U004  Manager  ...         293             340  non-encrypted\n",
      "4     P1005    U005    Admin  ...         407              92  non-encrypted\n",
      "\n",
      "[5 rows x 10 columns]\n",
      "Remaining data after dropping NaNs: 10 rows.\n",
      "Transformed DataFrame head:\n",
      "  policy_id user_id     role  ... login_count last_login_days      data_type\n",
      "0     P1001    U001  Manager  ...        NjY2            MzQ2      encrypted\n",
      "1     P1002    U002    Admin  ...          40              53  non-encrypted\n",
      "2     P1003    U003  Manager  ...         186             386  non-encrypted\n",
      "3     P1004    U004  Manager  ...         293             340  non-encrypted\n",
      "4     P1005    U005    Admin  ...         407              92  non-encrypted\n",
      "\n",
      "[5 rows x 10 columns]\n",
      "Sampled DataFrame head:\n",
      "  policy_id user_id     role  ... login_count last_login_days      data_type\n",
      "8     P1009    U009  Manager  ...         261             206  non-encrypted\n",
      "1     P1002    U002    Admin  ...          40              53  non-encrypted\n",
      "5     P1006    U006    Admin  ...        NjIw            MzY3      encrypted\n",
      "0     P1001    U001  Manager  ...        NjY2            MzQ2      encrypted\n",
      "7     P1008    U008     User  ...          33              35  non-encrypted\n",
      "\n",
      "[5 rows x 10 columns]\n",
      "Reshaped DataFrame head:\n",
      "        login_count  monthly_rate\n",
      "region                           \n",
      "APAC          408.4    158.000000\n",
      "EU            333.0    363.333333\n",
      "US            298.0    320.000000\n",
      "Data saved to '../data/transformed.csv', '../data/sampled_iam_policies.csv', and '../data/reshaped_iam_policies.csv'.\n",
      "Pipeline execution completed.\n"
     ]
    }
   ],
   "source": [
    "!python run_all.py --extract --transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2638c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "\n",
    "# TRANSFORM data\n",
    "\n",
    "# encode data\n",
    "def encode_data(data):\n",
    "    \"\"\"Simulate encryption by encoding the data using base64.\"\"\"\n",
    "    if pd.isna(data):  # Skip NaN values\n",
    "        print(f\"Skipping encryption for NaN value: {data}\")  # Debugging\n",
    "        return data\n",
    "    try:\n",
    "        encoded_data = base64.b64encode(str(data).encode('utf-8'))\n",
    "        return encoded_data.decode('utf-8')  # Return as a string\n",
    "    except Exception as e:\n",
    "        # Debugging\n",
    "        print(f\"Error encoding data: {data} ({e})\")\n",
    "        return data\n",
    "\n",
    "# Clean columns\n",
    "def clean_and_convert_column(df, column_name):\n",
    "    \"\"\"Helper function to clean and convert a column to numeric.\"\"\"\n",
    "    # Skip the conversion of encrypted values\n",
    "    if 'encrypted' in df['data_type'].values:\n",
    "        print(f\"Skipping numeric conversion for '{column_name}' due to 'encrypted' data type.\")\n",
    "    else:\n",
    "        # Convert to numeric, forcing errors if any to NaN, and display how many invalid entries are there.\n",
    "        df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
    "\n",
    "    # Count how many NaN values are there due to invalid values\n",
    "    invalid_count = df[column_name].isna().sum()\n",
    "    if invalid_count > 0:\n",
    "        print(f\"Warning: {invalid_count} invalid entries in column '{column_name}' have been converted to NaN.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# transform data\n",
    "def transform_data(df):\n",
    "    \"\"\"Transform the data based on sepecification\"\"\"\n",
    "    \n",
    "    print(f\"Initial DataFrame head:\\n{df.head()}\")  # Debugging: Check initial data\n",
    "\n",
    "    # **Reshape data first** (before encryption) to ensure numeric columns for reshaping\n",
    "    print(f\"Rows to be reshaped:\\n{df.head()}\")\n",
    "\n",
    "    # Check if there are enough data to reshape\n",
    "    print(f\"Unique regions before reshaping: {df['region'].nunique()}\")  # Number of unique regions\n",
    "    print(f\"Missing values in 'region' column before reshaping: {df['region'].isna().sum()}\")  # Check missing regions\n",
    "\n",
    "    # Only reshape if there is sufficient data\n",
    "    # Reshaping data is to change the structure of the dataset to make it suitable for analysis, reporting, or use by other systems\n",
    "    # Ensure there's enough data left for reshaping\n",
    "    # Pivoting is a reshaping operation that rotates data from rows into columns\n",
    "    # Aggregation is the process of calculating summary statistics for groups of records\n",
    "    if df.shape[0] > 1 and df['region'].nunique() > 1:\n",
    "        # Optional: Reshape data (like: pivoting or aggregating)\n",
    "        try:\n",
    "            # The columns are now numeric, we can safely compute the mean\n",
    "            print(f\"Attempting to reshape data with {df['region'].nunique()} unique regions.\")\n",
    "            df_reshaped = df.pivot_table(index=['region'], values=['monthly_rate', 'login_count'], aggfunc='mean')\n",
    "            print(f\"Reshaped DataFrame head:\\n{df_reshaped.head()}\")  # Check reshaped data\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reshaping: {e}\")\n",
    "            df_reshaped = pd.DataFrame()  # Create an empty DataFrame in case of error\n",
    "    else:\n",
    "        df_reshaped = pd.DataFrame()  # No data to reshape if we don't have enough rows\n",
    "\n",
    "    # **Encrypt the data** after reshaping\n",
    "    print(f\"Encrypting the data...\")\n",
    "\n",
    "    # Encrypt the fields where 'data_type' is 'encrypted' (i.e., simulate encryption)\n",
    "    df['monthly_rate'] = df.apply(\n",
    "        lambda row: encode_data(row['monthly_rate']) if row['data_type'] == 'encrypted' else row['monthly_rate'], axis=1)\n",
    "    \n",
    "    df['login_count'] = df.apply(\n",
    "        lambda row: encode_data(row['login_count']) if row['data_type'] == 'encrypted' else row['login_count'], axis=1)\n",
    "    \n",
    "    df['last_login_days'] = df.apply(\n",
    "        lambda row: encode_data(row['last_login_days']) if row['data_type'] == 'encrypted' else row['last_login_days'], axis=1)\n",
    "\n",
    "    # Debugging: Check data after encryption\n",
    "    print(f\"Data after encryption:\\n{df.head()}\")\n",
    "\n",
    "    # **Save the data before cleaning** and converting\n",
    "    df.to_csv('../data/transform_before_cleaning.csv', index=False)\n",
    "    print(f\"Data before cleaning and conversion saved to '../data/transform_before_cleaning.csv'\")\n",
    "\n",
    "    # **Clean and convert** the columns to numeric (but skip `encrypted` data)\n",
    "    df = clean_and_convert_column(df, 'monthly_rate')\n",
    "    df = clean_and_convert_column(df, 'login_count')\n",
    "    df = clean_and_convert_column(df, 'last_login_days')\n",
    "\n",
    "    print(f\"Data after cleaning and conversion to numeric:\\n{df.head()}\")  # Check data after conversion\n",
    "\n",
    "    # **Save the data after cleaning** but before dropping NaNs\n",
    "    df.to_csv('../data/transform_after_cleaning.csv', index=False)\n",
    "    print(f\"Data after cleaning and conversion saved to '../data/transform_after_cleaning.csv'\")\n",
    "\n",
    "    # **Check how many rows are being dropped** when we remove NaNs\n",
    "    print(f\"Data before dropping NaNs: {df.shape[0]} rows.\")\n",
    "    \n",
    "    # Remove rows where any of the numeric columns are NaN (check for missing data)\n",
    "    df = df.dropna(subset=['monthly_rate', 'login_count', 'last_login_days'])\n",
    "\n",
    "    print(f\"Data after dropping NaNs:\\n{df.head()}\")  # Check data after removing NaNs\n",
    "    print(f\"Remaining data after dropping NaNs: {df.shape[0]} rows.\")  # How many rows remain?\n",
    "\n",
    "    # **Sampling the data**\n",
    "    # Sampling the data is the process of selecting a subset of data \n",
    "    # from a large amount of dataset for analysis.\n",
    "    df_sampled = df.sample(frac=0.5, random_state=42)\n",
    "\n",
    "    # Debugging: Print transformed data\n",
    "    print(f\"Transformed DataFrame head:\\n{df.head()}\")  # Check the transformation\n",
    "    print(f\"Sampled DataFrame head:\\n{df_sampled.head()}\")  # Check sampled data\n",
    "    print(f\"Reshaped DataFrame head:\\n{df_reshaped.head()}\")  # Check reshaped data\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    df.to_csv('../data/transformed.csv', index=False)  # Save transformed data to 'transformed.csv'\n",
    "    df_sampled.to_csv('../data/sampled_iam_policies.csv', index=False)  # Save sampled data\n",
    "    df_reshaped.to_csv('../data/reshaped_iam_policies.csv', index=False)  # Save reshaped data\n",
    "\n",
    "    # Confirmation message for saved files\n",
    "    print(\"Data saved to '../data/transformed.csv', '../data/sampled_iam_policies.csv', and '../data/reshaped_iam_policies.csv'.\")\n",
    "\n",
    "    # Return transformed data, sampled data, and reshaped data\n",
    "    return df, df_sampled, df_reshaped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d3f01",
   "metadata": {},
   "source": [
    "ETL Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8437d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded transformed data from 'transformed.csv'.\n",
      "Running Load Step...\n",
      "Successfully connected to Supabase PostgreSQL.\n",
      "Data to be loaded:\n",
      "  policy_id user_id     role  ... login_count last_login_days      data_type\n",
      "0     P1001    U001  Manager  ...        NjY2            MzQ2      encrypted\n",
      "1     P1002    U002    Admin  ...          40              53  non-encrypted\n",
      "2     P1003    U003  Manager  ...         186             386  non-encrypted\n",
      "3     P1004    U004  Manager  ...         293             340  non-encrypted\n",
      "4     P1005    U005    Admin  ...         407              92  non-encrypted\n",
      "\n",
      "[5 rows x 10 columns]\n",
      "Inserting values: ('P1001', 'U001', 'Manager', 'Basic', nan, True)\n",
      "Inserting values: ('P1002', 'U002', 'Admin', 'Basic', 520.0, True)\n",
      "Inserting values: ('P1003', 'U003', 'Manager', 'Enterprise', 50.0, True)\n",
      "Inserting values: ('P1004', 'U004', 'Manager', 'Basic', 50.0, True)\n",
      "Inserting values: ('P1005', 'U005', 'Admin', 'Standard', 520.0, False)\n",
      "Inserting values: ('P1006', 'U006', 'Admin', 'Enterprise', nan, False)\n",
      "Inserting values: ('P1007', 'U007', 'Manager', 'Standard', 120.0, False)\n",
      "Inserting values: ('P1008', 'U008', 'User', 'Standard', 50.0, False)\n",
      "Inserting values: ('P1009', 'U009', 'Manager', 'Standard', 520.0, False)\n",
      "Inserting values: ('P10010', 'U0010', 'Manager', 'Standard', 50.0, True)\n",
      "Data loaded successfully into the Supabase database.\n",
      "Pipeline execution completed.\n"
     ]
    }
   ],
   "source": [
    "!python run_all.py --load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e77578bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Supabase PostgreSQL.\n",
      "Data to be loaded:\n",
      "  policy_id user_id     role   plan_type monthly_rate  premium\n",
      "0     P1001    U001  Manager       Basic         NTIw     True\n",
      "1     P1002    U002    Admin       Basic          520     True\n",
      "2     P1003    U003  Manager  Enterprise           50     True\n",
      "3     P1004    U004  Manager       Basic           50     True\n",
      "4     P1005    U005    Admin    Standard          520    False\n",
      "Inserting values: ('P1001', 'U001', 'Manager', 'Basic', nan, True)\n",
      "Inserting values: ('P1002', 'U002', 'Admin', 'Basic', 520.0, True)\n",
      "Inserting values: ('P1003', 'U003', 'Manager', 'Enterprise', 50.0, True)\n",
      "Inserting values: ('P1004', 'U004', 'Manager', 'Basic', 50.0, True)\n",
      "Inserting values: ('P1005', 'U005', 'Admin', 'Standard', 520.0, False)\n",
      "Inserting values: ('P1006', 'U006', 'Admin', 'Enterprise', nan, False)\n",
      "Inserting values: ('P1007', 'U007', 'Manager', 'Standard', 120.0, False)\n",
      "Inserting values: ('P1008', 'U008', 'User', 'Standard', 50.0, False)\n",
      "Inserting values: ('P1009', 'U009', 'Manager', 'Standard', 520.0, False)\n",
      "Inserting values: ('P10010', 'U0010', 'Manager', 'Standard', 50.0, True)\n",
      "Data loaded successfully into the Supabase database.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from psycopg2 import sql\n",
    "import os\n",
    "\n",
    "# Connect to Supabase PostgreSQL database\n",
    "def connect_to_supabase():\n",
    "    try:\n",
    "        # Use Supabase database connection string\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"db.prwzydmfrcbepgevmqmu.supabase.co\", # Supabase host\n",
    "            port=5432,  # Default PostgreSQL port\n",
    "            database=\"postgres\",  # Database name\n",
    "            user=\"etl_user1\",  # Database username\n",
    "            password=\"cyuqD639?TT\",  # Database password\n",
    "            sslmode=\"require\"  # SSL connection for security\n",
    "        )\n",
    "        print(\"Successfully connected to Supabase PostgreSQL.\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Unable to connect to database - {e}\")\n",
    "        return None\n",
    "\n",
    "# Load data to Supabase\n",
    "def load_data_to_supabase(df):\n",
    "    conn = connect_to_supabase()\n",
    "    \n",
    "    # If the connection is missing, the function stops immediately \n",
    "    # to prevent error.\n",
    "    if conn is None:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create a cursor object\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Supabase table schema\n",
    "        insert_query = sql.SQL(\"\"\"\n",
    "            INSERT INTO iam_policies \n",
    "            (policy_id, user_id, role, plan_type, monthly_rate, premium)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (policy_id) DO NOTHING;\n",
    "        \"\"\")\n",
    "\n",
    "        # Print the DataFrame structure\n",
    "        print(f\"Data to be loaded:\\n{df.head()}\")  # Debugging: Print the first few rows of the DataFrame\n",
    "\n",
    "        # Ensure that 'monthly_rate' is numeric and handle NaN values by converting them to None (NULL in DB)\n",
    "        df['monthly_rate'] = pd.to_numeric(df['monthly_rate'], errors='coerce')  # Convert to numeric, invalid values become NaN\n",
    "        df['monthly_rate'] = df['monthly_rate'].where(df['monthly_rate'].notna(), None)  # Replace NaN with None\n",
    "\n",
    "        # Loop through each row in the DataFrame and insert it into the PostgreSQL table\n",
    "        for _, row in df.iterrows():\n",
    "            # Ensure the values tuple is packed with the correct number of elements (6 values in total)\n",
    "            values = (\n",
    "                row['policy_id'],  # policy_id\n",
    "                row['user_id'],    # user_id\n",
    "                row['role'],       # role\n",
    "                row['plan_type'],  # plan_type\n",
    "                row['monthly_rate'],  # monthly_rate (now guaranteed to be numeric or None)\n",
    "                row['premium'],    # premium (True/False)\n",
    "            )\n",
    "\n",
    "            # Print the values being inserted to debug\n",
    "            print(f\"Inserting values: {values}\")  # Debugging: Print the tuple to see if it's correctly formatted\n",
    "            \n",
    "            # Execute the insert query with the values\n",
    "            cur.execute(insert_query, values)\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(f\"Data loaded successfully into the Supabase database.\")\n",
    "    # Catch the exception\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "# Main function for execution\n",
    "def main():\n",
    "    # Load the new transformed CSV to DataFrame\n",
    "    df = pd.read_csv('../data/transformed.csv')\n",
    "\n",
    "    # Ensure the DataFrame has the right columns that match the table schema\n",
    "    # Columns must be: policy_id, user_id, role, plan_type, monthly_rate, premium, etc\n",
    "    df = df[['policy_id', 'user_id', 'role', 'plan_type', 'monthly_rate', 'premium']]\n",
    "\n",
    "    load_data_to_supabase(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df75bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
